{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from transformers.generation.utils import GenerationConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "instruction = '''以下是关于医学知识的单项选择题，请根据题目输出唯一的答案选项\\n'''\n",
    "instruction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_file_data = '../data/MedQA/Mainland/test.jsonl'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_dict_test = []\n",
    "with open(path_file_data, 'r', encoding=\"utf-8\") as f:\n",
    "    for idx, line in enumerate(f):\n",
    "        data = json.loads(line)\n",
    "        data['ID'] = idx\n",
    "        data['A'], data['B'], data['C'], data['D'], data['E'] = data['options']['A'], data['options']['B'], data['options']['C'], data['options']['D'], data['options']['E']\n",
    "        del data['options']\n",
    "        list_dict_test.append(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_dict_test[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, dict_test in enumerate(tqdm(list_dict_test)):\n",
    "    question, answer = dict_test['question'], dict_test['answer']\n",
    "    a, b, c, d, e = dict_test['A'], dict_test['B'], dict_test['C'], dict_test['D'], dict_test['E']\n",
    "    question = question.replace(\"（ ）。\", \"\")\n",
    "    input = instruction + f\"问题：{question}: (A){a}, (B){b}, (C){c}, (D){d}, (E){e}\\n\" + \"答案：\"\n",
    "    dict_test['Input'] = input\n",
    "    # print(input, '\\t' ,answer)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_dir_model = 'path_of_baichuan2_model'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(path_dir_model, use_fast=False, trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = AutoModelForCausalLM.from_pretrained(path_dir_model, device_map=\"auto\", trust_remote_code=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(path_dir_model, device_map=\"auto\", torch_dtype=torch.float16, trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.generation_config = GenerationConfig.from_pretrained(path_dir_model)\n",
    "model.generation_config.max_new_tokens = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_response(text, flag_print=True):\n",
    "    messages = []\n",
    "    messages.append({\"role\": \"user\", \"content\": text})\n",
    "    with torch.no_grad():\n",
    "        response = model.chat(tokenizer, messages)\n",
    "        if flag_print:\n",
    "            print(text)\n",
    "            print('-------------------------------')\n",
    "            print(response)\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = get_response(dict_test['Input'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name_task = \"Prompt-bc2-7B\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_dict_test[0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name_task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch \n",
    "batch_size = 6\n",
    "num_batch = int(np.ceil(len(list_dict_test) / batch_size))\n",
    "\n",
    "with torch.no_grad():\n",
    "    for idx_batch in tqdm(range(num_batch)):\n",
    "        # tokenize  \n",
    "        list_data_batch = list_dict_test[idx_batch*batch_size: (idx_batch+1)*batch_size]\n",
    "        list_input_batch = [ dict_one['Input'] for dict_one in list_data_batch]\n",
    "\n",
    "        # try:\n",
    "        input_id_batch = tokenizer.batch_encode_plus(list_input_batch, padding=True, truncation=True)['input_ids']\n",
    "        # for chat\n",
    "        input_id_batch = [\n",
    "            [model.generation_config.user_token_id]\n",
    "            + input_id\n",
    "            + [model.generation_config.assistant_token_id]\n",
    "            for input_id in input_id_batch\n",
    "        ]\n",
    "        input_id_batch = torch.LongTensor(input_id_batch).to(model.device)\n",
    "        # generate\n",
    "        output_batch = model.generate(input_id_batch, generation_config=model.generation_config)\n",
    "        # response\n",
    "        for dict_one, input_id, output in zip(list_data_batch, input_id_batch, output_batch):\n",
    "            dict_one[\"Result\"] = tokenizer.decode(\n",
    "                output[len(input_id):], skip_special_tokens=True\n",
    "            )\n",
    "        # except:\n",
    "        #     print(f\"Error: {idx}\")\n",
    "        # save\n",
    "        # if idx != 0 or idx % num_save == 0:\n",
    "        #     with open(path_file_dataset, \"w\", encoding=\"utf-8\") as f:\n",
    "        #         json.dump(list_dict_test, f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save list_dict_test into json\n",
    "with open(f'../result/{name_task}.json', \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(list_dict_test, f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(list_dict_test), len([dict_test for dict_test in list_dict_test if dict_test.get('Result')])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prediction(result_pred, list_option_name):\n",
    "    list_str_split = ['最终答案是','最终答案为','最可能的选项是','最可能的诊断是','正确的选项是', '正确选项是', '正确选项为','最可能的答案是','最有可能的答案是']\n",
    "    for str_split in list_str_split:\n",
    "        if len(result_pred.split(str_split))>1:\n",
    "            result_pred = result_pred.split(str_split)[-1]\n",
    "            result_pred = result_pred.split('。')[0]\n",
    "            break\n",
    "    list_option_idx = [ chr(ord('A') + idx) for idx in range(len(list_option_name))]\n",
    "    list_option_idx_name = [ [option_idx, option_name] for option_idx, option_name in zip(list_option_idx, list_option_name)]\n",
    "    list_option_idx_name = sorted(list_option_idx_name, key=lambda x: len(x[1]), reverse=True)\n",
    "    list_option_match = []\n",
    "    # search option_idx and option_name\n",
    "    for option_idx, option_name in list_option_idx_name:\n",
    "        if option_idx in result_pred or option_name in result_pred:\n",
    "            list_option_match.append(option_idx)\n",
    "            if option_name in result_pred:\n",
    "                result_pred = result_pred.replace(option_name, '')\n",
    "    if len(list_option_match)==1:\n",
    "        return list_option_match[0]\n",
    "    elif len(list_option_match)>1:\n",
    "        return 'Null'\n",
    "    else:\n",
    "        return 'Null'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_dict_result_correct, list_dict_result_wrong, list_dict_result_none = [], [], []\n",
    "for idx, dict_test in enumerate(tqdm(list_dict_test)):\n",
    "    list_option_name = [dict_test['A'], dict_test['B'], dict_test['C'], dict_test['D'], dict_test['E']]\n",
    "    dict_test['Prediction'] = get_prediction(dict_test['Result'], list_option_name)\n",
    "    if dict_test['Prediction']=='Null':\n",
    "        list_dict_result_none.append(dict_test)\n",
    "    elif dict_test['Answer'] == dict_test['Prediction']:\n",
    "        list_dict_result_correct.append(dict_test)\n",
    "    else:\n",
    "        list_dict_result_wrong.append(dict_test)\n",
    "    # if len(dict_test['Result']) > 6:\n",
    "    #     print(dict_test['Result']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_correct = len(list_dict_result_correct)\n",
    "count_wrong = len(list_dict_result_wrong)\n",
    "count_none = len(list_dict_result_none)\n",
    "acc = round(len(list_dict_result_correct) / len(list_dict_test)*100, 2)\n",
    "print(f\"Accuracy is {acc}% with {count_correct} correct, {count_wrong} wrong, and {count_none} none\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Too long 111"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'../result/{name_task}.json', 'w', encoding=\"utf-8\") as f:\n",
    "    f.write(json.dumps(list_dict_test, indent=4, ensure_ascii=False))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'../result/{name_task}.json', 'r', encoding=\"utf-8\") as f:\n",
    "    list_dict_test_sample = json.load(f)\n",
    "with open('../data/list_test_knowledge_idx.txt', 'r') as f:\n",
    "    list_test_knowledge_idx = f.readlines()\n",
    "with open('../data/list_test_example_idx.txt', 'r') as f:\n",
    "    list_test_example_idx = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_test_knowledge_idx = [ int(idx.strip()) for idx in list_test_knowledge_idx if idx.strip() ]\n",
    "list_test_example_idx = [ int(idx.strip()) for idx in list_test_example_idx if idx.strip() ]\n",
    "list_dict_test_knowledge = [ list_dict_test_sample[idx] for idx in list_test_knowledge_idx ]\n",
    "list_dict_test_example = [ list_dict_test_sample[idx] for idx in list_test_example_idx ]\n",
    "print(len(list_dict_test_sample), len(list_dict_test_knowledge), len(list_dict_test_example))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_performance(list_dict_test_sample):\n",
    "    list_dict_result_correct, list_dict_result_wrong, list_dict_result_none = [], [], []\n",
    "    for idx, dict_test in enumerate(list_dict_test_sample):\n",
    "        if dict_test['Prediction']=='Null':\n",
    "            list_dict_result_none.append(dict_test)\n",
    "        elif dict_test['Answer'] == dict_test['Prediction']:\n",
    "            list_dict_result_correct.append(dict_test)\n",
    "        else:\n",
    "            list_dict_result_wrong.append(dict_test)\n",
    "    count_correct = len(list_dict_result_correct)\n",
    "    count_wrong = len(list_dict_result_wrong)\n",
    "    count_none = len(list_dict_result_none)\n",
    "    acc = len(list_dict_result_correct) / len(list_dict_test_sample)\n",
    "    print(f\"Accuracy is {round(acc*100, 2)}% with {count_none} none, {count_correct} correct, and {count_wrong} wrong， total {len(list_dict_test_sample)}\")\n",
    "    return round(acc, 4), count_none, count_correct, count_wrong"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_MK, count_none_MK, _, _ = cal_performance(list_dict_test_knowledge)\n",
    "acc_CA, count_none_CA, _, _ = cal_performance(list_dict_test_example)\n",
    "acc_all, count_none_all, _, _ = cal_performance(list_dict_test_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(count_none_MK, acc_MK, count_none_CA, acc_CA, count_none_all, acc_all, sep='\\t')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## End"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.13 ('nlp')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e0dda50c8f3106409627f3c7388c0a3de156665d66c9593dafc60667d2d1be47"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
